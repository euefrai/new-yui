# Core Engine â€” Arquitetura da Yui

A Yui nÃ£o Ã© chatbot nem IDE. Ã‰ uma **mini DevOS** baseada em IA:
**Workspace + Runtime + MemÃ³ria + Agente**.

## Arquitetura

```
[UsuÃ¡rio] â†’ Agent Controller
                â†“
           [model="auto" â†’ Persona Router + Arbitration Engine]
                â†“
           Action Engine â†’ Context Kernel â†’ Persona (Yui/Heathcliff/Hybrid)
                â†“                â†“                â†“
           [editor]        [arquivos]        [tools]
           [terminal]      [erros]          [RAG]
           [analyzer]      [chat]           [executor]
           [RAG]           [workspace]
```

## Event Bus â€” Sistema Nervoso Central

```
        emit                    on
Planner â”€â”€â”€â”€â”€â”€â”€â”€â–º Event Bus â—„â”€â”€â”€ Reflection Loop
  Task Engine â”€â”€â”€â”€â”€â”€â”€â”€â–º Event Bus â—„â”€â”€â”€ Observability
  Reflection â”€â”€â”€â”€â”€â”€â”€â”€â–º Event Bus â—„â”€â”€â”€ UI (stream)
```

Antes: Planner chama Task â†’ Task chama Context â†’ acoplamento.
Depois: mÃ³dulos emitem e escutam; nenhum chama outro direto.

## MÃ³dulos

### 1. Action Engine (`core/engine/action_router.py`)

O coraÃ§Ã£o do orquestrador. Decide **quando**:
- abrir editor
- executar cÃ³digo
- usar terminal
- chamar analyzer
- fazer RAG

```python
from core.engine import route_action

intent = route_action("executar o main.py", last_tool="analisar_projeto")
# intent.action = "execute_code"
# intent.tool_hint = "execute_sandbox"
```

### 2. Arbitration Engine (`core/arbitration_engine.py`)

**Decision Layer** â€” decide qual persona lidera (Yui, Heathcliff ou Hybrid).

- model="auto" â†’ heurÃ­stica decide: refatorar/backend/arquitetura â†’ Heathcliff; ui/texto/fluxo â†’ Yui; else â†’ Hybrid
- model="yui" | "heathcliff" â†’ preferÃªncia manual
- Reduz inconsistÃªncia; Planner e Self-Critic ficam mais previsÃ­veis

```python
from core.arbitration_engine import decide_leader

arb = decide_leader("refatorar o backend", has_console_errors=True)
# arb.leader = "heathcliff"
```

### 3. Context Kernel (`core/engine/context_kernel.py`)

Unifica o contexto em tempo real:
- arquivos ativos
- erros do console
- histÃ³rico do chat
- estado do workspace

```python
from core.engine import get_context_snapshot, snapshot_to_prompt

snapshot = get_context_snapshot()
prompt_text = snapshot_to_prompt(snapshot)
```

### 4. Execution Graph Engine (`core/execution_graph.py`)

**Orquestrador de fluxo** â€” toda aÃ§Ã£o vira um nÃ³ observÃ¡vel.

- Input â†’ Planner cria mini-fluxo â†’ Nodes executam â†’ Observer acompanha â†’ Critic valida
- Permite: pausar, reexecutar nÃ³, progresso visual, custo por etapa
- Emite eventos: `execution_node_start`, `execution_node_done`, `execution_node_failed`

```python
from core.execution_graph import ExecutionGraph, Node, graph_from_planner_steps

# Exemplo: "Cria API e gera ZIP"
g = ExecutionGraph(intention="criar_api_zip")
g.add_step("Parse Intent", lambda ctx: ctx)
g.add_step("Generate Files", lambda ctx: {"files": [...]})
# Zip Builder usa Task Scheduler (background); link [DOWNLOAD] aparece logo
g.add_step("Zip Builder", lambda ctx: "/download/projeto.zip")
g.add_step("Download Link", lambda ctx: ctx.get("_result_Zip Builder"))

result = g.run(ctx={"user_message": "cria API de login"})
status = g.to_ui_status()  # [{"name": "Parse Intent", "status": "done", "symbol": "âœ“"}, ...]
```

### 5. System State Engine (`core/system_state.py`)

**CÃ©rebro de estado** â€” consistÃªncia operacional.

- `mode`, `workspace_open`, `executing_graph`, `terminal_sessions_alive`
- `should_enable_editor_features()`: sÃ³ quando workspace aberto
- `should_activate_observer()`: sÃ³ quando Execution Graph rodando

```python
from core.system_state import get_state, should_enable_editor_features

if should_enable_editor_features():
    enable_editor_features()
```

### 6. Resource Governor (`core/resource_governor.py`)

**Controle inteligente de recursos** â€” decide "posso executar isso agora?".

- `allow_preview(ram)` â€” preview sÃ³ se RAM < 75%
- `allow_heavy_agent(cpu, ram)` â€” agent pesado sÃ³ se CPU < 85%, RAM < 80%
- `allow_planner(cpu)` â€” planner sÃ³ se CPU < 90%
- `allow_watchers(ram)` â€” watchers sÃ³ se RAM < 70%

```python
from core.resource_governor import get_governor

if get_governor().allow_preview().allow:
    start_live_preview()
```

### 7. Event Bus (`core/event_bus.py`)

**Sistema Nervoso Central** â€” nenhum mÃ³dulo chama outro direto. Tudo vira eventos.

Sem Event Bus: Planner chama Task â†’ Task chama Context â†’ emaranhado.
Com Event Bus: Planner emite | Task emite | Reflection escuta | UI escuta.

| API | DescriÃ§Ã£o |
|-----|-----------|
| `on(event, fn)` / `subscribe(event, fn)` | Registra listener |
| `emit(event, **kwargs)` | Dispara evento para todos os listeners |
| `unsubscribe(event, fn)` | Remove listener |
| `list_events()` | Lista eventos conhecidos |

**Eventos principais:**

| Evento | Payload | Quem emite | Quem escuta |
|--------|---------|------------|-------------|
| `workspace_toggled` | open: bool | routes | system_state |
| `file_changed` | path, action | routes | â€” |
| `task_iniciada` | task, task_id, fn_name | task_engine | observability |
| `task_queued` | task_id, fn_name | task_engine | observability |
| `task_done` | task_id, result | task_engine | observability |
| `task_failed` | task_id, error | task_engine | observability |
| `task_finished` | task_id, duration, success, ... | task_engine | reflection_loop |
| `erro_detectado` | source, error | task_engine | observability |
| `memoria_alta` | ram_mb, threshold | reflection_loop | observability |
| `memory_update_requested` | root? | web_server | scheduler |
| `agent_requested` | model, user_message | agent_controller | â€” |
| `zip_ready` | download_url | tools_runtime | observability |

```python
from core.event_bus import emit, on

emit("workspace_toggled", open=True)
on("file_changed", lambda path, action: invalidate_cache(path))
on("task_iniciada", lambda task, **kw: ui_stream(task))
on("memoria_alta", lambda ram_mb, **kw: log_warning(f"RAM {ram_mb}MB"))
```

### 8. Event Wiring (`core/event_wiring.py`)

Conecta eventos aos mÃ³dulos no startup. Chamado por `web_server.py`.

- `workspace_toggled` â†’ `system_state.set_workspace_open()`
- `memory_update_requested` â†’ `scheduler.add(indexar_projeto)`
- `task_finished` â†’ `reflection_loop.avaliar_e_armazenar()`

### 9. Task Engine (`core/task_engine.py`)

**CÃ©rebro operacional** â€” em vez de `if comando == "editar_arquivo": editar()`, tudo vira task.

- `registrar(nome, func)` â€” registra tarefa
- `executar(nome, *args, **kwargs)` â€” executa e rastreia
- `executar_tool(tool_name, args)` â€” executa tool com status (usado pelo agent_controller)
- `get_active()` â€” tarefas em execuÃ§Ã£o (para UI)
- `get_summary()` â€” `{active_count, summary_text}` â€” ex: "editando 2 arquivos, gerando ZIP"

Permite: Heathcliff planejar vÃ¡rias aÃ§Ãµes; logar custo por tarefa; UI "ğŸŸ¡ Heathcliff estÃ¡ editando 3 arquivos...".

```python
from core.task_engine import get_task_engine

engine = get_task_engine()
engine.registrar("minha_acao", minha_funcao)
result = engine.executar("minha_acao", dados=...)
summary = engine.get_summary()  # {"summary_text": "editando arquivo(s)"}
```

### 10. Action Planner (`core/action_planner.py`)

**Mini arquiteto interno** â€” nÃ£o executa nada. SÃ³ responde: QUAL sequÃªncia de tarefas precisa acontecer.

- `inferir_intencao(mensagem)` â€” intenÃ§Ã£o heurÃ­stica
- `planejar(intencao)` â€” retorna `[{"task", "label", "tool"}, ...]`
- `get_label_for_tool(tool_name)` â€” label para UI (ex: "ğŸ“ Criando arquivos...")
- Fluxo: Intent Parser â†’ Planner â†’ Task Engine â†’ Streaming UI

Evita: IA tenta fazer tudo de uma vez â†’ CPU sobe, RAM sofre, SIGKILL.
Permite: Yui pensa antes de agir, executa em etapas pequenas.

```python
from core.action_planner import get_action_planner

planner = get_action_planner()
intencao = planner.inferir_intencao("cria uma calculadora")
passos = planner.planejar(intencao)  # [{"task": "criar_estrutura", "label": "ğŸ“ Criando estrutura..."}, ...]
for passo in passos:
    if passo.get("tool"):
        task_engine.executar_tool(passo["tool"], args)
```

Streaming: emite `__STATUS__:planejando`, `__STATUS__:executing_tools:ğŸ“ Criando arquivos...`.

### 11. Context Engine (`core/context_engine.py`)

**MemÃ³ria operacional (RAM)** â€” nÃ£o vai pro banco nem pro RAG. SÃ³ existe enquanto a sessÃ£o estÃ¡ viva.

- `get_context(user_id, chat_id)` â€” engine por sessÃ£o
- `ctx.set(chave, valor)` / `ctx.get(chave)` â€” estado operacional
- Chaves: `modo`, `arquivo_aberto`, `task_ativa`, `ultimo_erro`, `workspace_open`
- `to_prompt_snippet()` â€” injeta no prompt da IA
- `update_from_snapshot()` â€” atualiza a partir de workspace_open, active_files

Evita: IA esquece workspace, Planner recalcula tudo, CPU sobe.
Permite: Heathcliff "lembra" o que estÃ¡ fazendo.

```python
from core.context_engine import get_context, update_from_snapshot

ctx = get_context(user_id, chat_id)
ctx.set("modo", "workspace")
ctx.set("arquivo_aberto", "app.py")
snippet = ctx.to_prompt_snippet()  # "[Estado operacional] Modo atual: workspace. Arquivo aberto: app.py."
```

Fluxo: Intent â†’ Context Engine â†’ Planner â†’ Task

### 12. Workspace Indexer (`core/workspace_indexer.py`)

**Mapa mental do projeto** â€” snapshot leve. NÃƒO Ã© RAG. NÃƒO Ã© memÃ³ria longa.

- `scan(base_path?)` â€” retorna `{python: [...], html: [...], css: [...], js: [...], total, extensoes}`
- `to_prompt_snippet(mapa)` â€” injeta no prompt: "Projeto: 12 Python, 3 HTML. Ãšltimo editado: app.py."
- `should_split_task(mapa, threshold?)` â€” sugere dividir quando muitos arquivos

IntegraÃ§Ã£o: quando workspace abre, Context Engine faz scan e armazena `workspace_map`.
API: `GET /api/system/workspace_index?base=sandbox`

### 13. Execution Guard (`core/execution_guard.py`)

**Resource Manager** â€” vigia CPU, RAM antes de executar tarefas. Evita SIGKILL.

- `memoria_ok(limite_mb?)` â€” RAM usada < limite
- `cpu_ok(limite?)` â€” CPU % < limite
- `pode_executar()` â€” RAM e CPU ok
- `wait_if_needed()` â€” espera atÃ© ok ou timeout (30s)

Fluxo: Planner â†’ Execution Guard â†’ Task Engine
Se recursos altos: guard espera atÃ© normalizar ou timeout.

```python
from core.execution_guard import get_guard

guard = get_guard()
if guard.pode_executar().ok:
    task_engine.executar("criar_projeto_arquivos", ...)
else:
    guard.wait_if_needed()  # ou time.sleep(1)
```

Env: `YUI_GUARD_RAM_MB=1500`, `YUI_GUARD_CPU_PCT=85`, `YUI_GUARD_RAM_PCT=85` (alternativo %).

### 13.1 Reflection Loop (`core/reflection_loop.py`)

**AutoavaliaÃ§Ã£o interna** â€” Planeja â†’ Executa â†’ Avalia â†’ Ajusta.

ApÃ³s cada task, telemetria leve (tempo, RAM, sucesso) Ã© avaliada e retorna estado:
- `ok` â€” normal
- `modo_economia` â€” RAM > limite â†’ Planner reduz steps, evita zip
- `dividir_tasks` â€” tempo > limite â†’ Planner usa steps menores

O Planner e o Context Engine leem `get_estado_reflexao()` e adaptam. Sem IA extra; sÃ³ telemetria inteligente.

```python
from core.reflection_loop import get_reflection_loop, get_estado_reflexao

loop = get_reflection_loop()
estado = loop.avaliar({"memoria": 1100, "tempo": 1.2, "sucesso": True})  # "ok"
estado = get_estado_reflexao()  # Ãºltimo armazenado
```

Env: `YUI_REFLECTION_RAM_MB=1200`, `YUI_REFLECTION_TEMPO_SEC=3`

### 13.2 Persona Router (`core/persona_router.py`)

**CÃ©rebro que decide quem age** â€” Intent â†’ Context â†’ Persona Router â†’ Planner.

NÃ£o Ã© "uma IA com duas skins". Ã‰ duas inteligÃªncias cooperando:
- **Yui**: criativo, UX, suporte, respostas amplas
- **Heathcliff**: engenharia, cÃ³digo, respostas tÃ©cnicas e concisas

Integra com Reflection Loop: RAM alta â†’ Heathcliff assume (respostas mais curtas).

```python
from core.persona_router import decidir_persona, get_persona_router

decision = decidir_persona("refatorar_codigo", {"modo": "workspace", "estado_reflexao": "ok"})
# decision.persona = "heathcliff"
# decision.reason = "Intent de engenharia: refatorar_codigo."
ctx.set("persona_ativa", decision.persona)
```

LÃ³gica: preferÃªncia do usuÃ¡rio > modo economia > workspace + intent > intent explÃ­cita > fallback Yui.

### 14. Capability Loader (`core/capability_loader.py`)

**Sistema de plugins dinÃ¢micos** â€” qualquer `cap_*.py` em `core/capabilities/` vira skill automaticamente.

- `carregar_capabilities(task_engine)` â€” escaneia e chama `register(task_engine)` em cada mÃ³dulo
- `list_loaded()` â€” capabilities jÃ¡ carregadas
- Sem hardcode. Core = estÃ¡vel. Capabilities = experimentais.

```python
# core/capabilities/cap_editor.py
def register(task_engine):
    task_engine.registrar("editar_arquivo", editar_arquivo)
```

Startup: `ğŸ” Capabilities carregadas: âœ” cap_editor âœ” cap_zip âœ” cap_analysis âœ” cap_web`
API: `GET /api/system/capabilities`

### 15. Task Scheduler (`core/task_scheduler.py`)

**AÃ§Ãµes assÃ­ncronas** â€” separa imediato vs fila vs background.

- `add(fn, data)` â†’ entra na fila, executa em background
- `add_now(fn, data)` â†’ executa em thread separada (nÃ£o bloqueia)
- Eventos: `task_queued`, `task_done`, `task_failed`
- **Usos**: indexaÃ§Ã£o RAG, geraÃ§Ã£o de ZIP (nÃ£o bloqueia o chat)

```python
from core.task_scheduler import get_scheduler

get_scheduler().add(lambda d: indexar_projeto(d), data=raiz)
```

### 16. Pending Downloads (`core/pending_downloads.py`)

URLs de arquivos gerados em background (ex: ZIP). Frontend faz poll para saber quando estÃ¡ pronto.

- `add_ready(url)` â€” registra download pronto
- `get_recent(since?)` â€” retorna URLs prontas (TTL 5 min)

### 17. Skill Registry (`core/skills/registry.py`)

**Registro dinÃ¢mico de habilidades** â€” Router NÃƒO conhece agentes. Router consulta Registry.

- `register(name, agent, tags, skip_planner?)` â€” registra skill
- `find(capability_type)` â€” retorna Skill ou None (match por tag)
- `list_skills()` â€” skills ativas para UI (auto-descoberta)

Bootstrap padrÃ£o: code-edit, analysis, general, memory-search, live-preview, terminal-exec, zip-builder.

```python
from core.skills.registry import find_skill, register_skill, list_skills

skill = find_skill("code_generation")  # â†’ Skill(agent="heathcliff", ...)
register_skill("design-mockup", "design_agent", ["design", "ui"])  # plugin novo
```

### 18. Confidence Engine (`core/router/confidence_engine.py`)

**Roteamento probabilÃ­stico** â€” ranking de agentes por score.

- `score(intent, skills)` â†’ ranking decrescente
- `best_agent(ranked, threshold=0.4)` â†’ agente ou fallback (yui)
- Criterios: match por tag (+0.6), contexto (+0.2), priority (+0.2)

Evita ativar mÃ³dulos pesados sem necessidade. Prepara terreno para multi-agente.

```python
from core.router.confidence_engine import get_confidence_engine, Intent
from core.skills.registry import get_all_skills

engine = get_confidence_engine()
ranked = engine.score(Intent(type="code_generation"), get_all_skills())
best = engine.best_agent(ranked)  # fallback se score < 0.4
```

### 19. Capability Router (`core/capability_router.py`)

**Roteador de habilidades** â€” decide qual mÃ³dulo resolve antes do planner.

- `route(user_message, intention?, action?, tool_hint?)` â†’ RouteDecision
- **Confidence Engine** rankeia skills; retorna melhor ou fallback
- Fallback: mapping padrÃ£o quando registry nÃ£o tem match

Integrado no agent_controller antes do planner. Registra routing no Observability.

```python
from core.capability_router import route, get_routing_display

dec = route(user_message, intention="analisar_projeto")
# dec.target = "heathcliff", dec.skip_planner = True (via confidence engine)
```

### 20. Observability Layer (`core/observability.py`)

**ConsciÃªncia interna** â€” rastreamento de aÃ§Ãµes e timeline.

- `trace(name, meta?)` â€” context manager para medir execuÃ§Ã£o
- `record_activity(kind, label, detail)` â€” registra para System Activity
- `get_timeline()` â€” spans com duraÃ§Ã£o (ms)
- `get_system_activity()` â€” atividade recente (graph, task, governor, event)
- `wire_observability()` â€” conecta Event Bus, Scheduler, Governor

Integrado em: Execution Graph (Trace por nÃ³), Task Scheduler (Trace por task), Resource Governor (bloqueios).

### 21. Sandbox Executor (`core/sandbox_executor/runner.py`)

ExecuÃ§Ã£o isolada â€” anti-SIGKILL:
- subprocess isolado
- timeout configurÃ¡vel
- limite de RAM (Unix)
- nÃ£o roda no worker principal

```python
from core.sandbox_executor import run_code

result = run_code("print(1+1)", lang="python", timeout=30)
```

### 22. Plugin Loader (`core/plugins_loader.py`)

- **scan**: descobre plugins em `plugins/`
- **register**: registra tools no `tools_registry`
- **inject**: disponibiliza ao engine no startup

```python
from core.plugins_loader import inject_into_engine

tools = inject_into_engine()  # lista de tools disponÃ­veis
```

## IntegraÃ§Ã£o

- **API**: `/api/sandbox/execute` usa `run_code` do sandbox_executor
- **Startup**: `web_server.py` chama `wire_events()` (event_wiring) e `inject_into_engine()`
- **Agent**: `action_router` e `context_kernel` podem ser usados no `agent_controller` para enriquecer contexto e decisÃµes

## Fluxo Capability Router + Confidence Engine

```
user_message
    â†’ Action Engine (route_action)
    â†’ Capability Router (route) â€” heurÃ­sticas â†’ capability_type
    â†’ Skill Registry (get_all) â€” todas as skills
    â†’ Confidence Engine (score) â€” ranking por tag + contexto + priority
    â†’ best_agent(threshold=0.4) â€” melhor ou fallback (yui)
    â†’ skip_planner? â†’ max_steps=2
    â†’ Planner (se habilitado)
    â†’ IA (Heathcliff/Yui/RAG/Execution Graph)
```

Roteamento probabilÃ­stico. Threshold evita ativar agente errado. Menos RAM, menos SIGKILL.

Routing exibido em System Activity: "â†’ Heathcliff (Engineering) (skip planner)"

## API System (`/api/system/*`)

| Endpoint | DescriÃ§Ã£o |
|----------|-----------|
| `GET /health` | CPU, RAM, modo (normal/fast/critical) |
| `GET /telemetry` | Custo acumulado, energia |
| `GET /guard` | Execution Guard â€” can_execute, ram_used_mb, cpu_percent |
| `GET /capabilities` | Capability Loader â€” capabilities carregadas |
| `GET /workspace_index` | Workspace Indexer â€” mapa do projeto (?base=sandbox) |
| `GET /governor` | allow_preview, allow_planner, allow_heavy_agent |
| `GET /scheduler` | queue_size da fila de tarefas |
| `GET /observability` | timeline (spans com ms) + activity (System Activity) |
| `GET /skills` | Skill Registry â€” skills ativas (auto-descoberta) |
| `GET /tasks/active` | Task Engine â€” tarefas em execuÃ§Ã£o, summary_text para UI |
| `GET /pending_downloads` | URLs de downloads prontos (?since=timestamp) |
| `GET /state` | mode, workspace_open, executing_graph |
| `GET /execution` | NÃ³s do Execution Graph para UI |
| `POST /events` | Frontend emite: workspace_toggled, file_changed, preview_started |

## Fluxo assÃ­ncrono (Scheduler)

```
evento                    â†’  listener              â†’  aÃ§Ã£o
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
memory_update_requested   â†’  event_wiring          â†’  scheduler.add(indexar)
criar_projeto_arquivos    â†’  agent_controller      â†’  run_tool(criar_zip, background=True)
task_done (zip)           â†’  _run_zip interno      â†’  add_ready(url), emit(zip_ready)
frontend [DOWNLOAD]       â†’  poll pending_downloadsâ†’  mostra "âœ“ Baixar Projeto"
```

**Antes**: Yui pensa â†’ trava â†’ responde  
**Depois**: Yui responde â†’ continua trabalhando em silÃªncio

## API Leve + Worker

### AI Loader (Lazy Loading) â€” `core/ai_loader.py`

NÃ£o carrega OpenAI, planner, agent no start do servidor. Carrega sÃ³ na primeira requisiÃ§Ã£o.

- `get_agent_controller()` â€” agent_controller
- `get_gerar_titulo_chat()` â€” tÃ­tulo do chat
- `get_detect_intent()`, `get_tool_executor()`, `get_session_memory()`

Reduz memÃ³ria inicial e evita SIGKILL no startup.

### Response Cache â€” `core/response_cache.py`

Cache de respostas curtas (oi, obrigado, etc). Evita chamar IA de novo.

- `get(prompt)` â€” retorna cache ou resposta padrÃ£o
- `set(prompt, response)` â€” armazena
- `should_cache(prompt)` â€” prompts â‰¤ 80 chars
- Respostas padrÃ£o para: oi, olÃ¡, obrigado, tchau, etc

### Session Manager â€” `core/session_manager.py`

SessÃ£o inteligente â€” pensamento atual por usuÃ¡rio (RAM).

- `get_session(user_id, chat_id?)` â€” sessÃ£o do usuÃ¡rio
- `update_session(user_id, data, chat_id?)` â€” atualiza
- `get_contexto(user_id, chat_id?)` â€” contexto pronto para prompt
- `append_turn(user_id, user_msg, assistant_msg, chat_id?)` â€” adiciona troca
- `clear_session(user_id, chat_id?)` â€” limpa (chamado em clear_chat)

MemÃ³ria = banco histÃ³rico. SessÃ£o = pensamento atual. Menos tokenizaÃ§Ã£o, respostas mais rÃ¡pidas.

### Job Queue â€” `core/job_queue.py`

Quando `USE_ASYNC_QUEUE=true`:

- `POST /api/send` com `async: true` â†’ retorna `{job_id}`
- `GET /api/chat/job/<job_id>` â†’ poll atÃ© `{status: "done", result: "..."}`

API fica leve; processamento pesado no worker (task_scheduler).

## Streaming de Resposta (SSE)

- **Rota**: `POST /api/chat/stream` â€” SSE (Server-Sent Events)
- **Formato**: `data: {json}\n\n` â€” chunks de texto ou `__STATUS__:thinking|planejando|executing_tools|executing_tools:ğŸ“ Criando arquivos...|done`
- **Frontend**: fetch + ReadableStream, parseia eventos, renderiza token por token
- **Chunks**: tamanho 12 chars (reduz pico de RAM, streaming mais fluido)
- **Status**: "ğŸ§  Pensando...", "ğŸ”§ Executando ferramentas...", "ğŸ” Analisando cÃ³digo..."

Resposta em tempo real, sem travar o servidor. Menos buffer, menos SIGKILL.

## PrÃ³ximos passos

1. **Frontend**: enviar `active_files` e `console_errors` no chat para o Context Kernel
2. **Agent**: usar `route_action` para sugerir tool_hint ao planner
3. **Plugins**: adicionar mais plugins em `plugins/`; suportam `--list` e `invoke`
4. **Streaming**: OpenAI `stream=True` para respostas diretas (mode answer)
